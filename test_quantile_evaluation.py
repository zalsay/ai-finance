#!/usr/bin/env python3
"""
æµ‹è¯•åˆ†ä½æ•°è¯„ä¼°åŠŸèƒ½
"""
import sys
import os
import pandas as pd
import numpy as np

# æ·»åŠ timesfmè·¯å¾„
timesfm_dir = os.path.join(os.path.dirname(__file__), 'timesfm')
sys.path.append(timesfm_dir)

from predict_chunked_functinos import predict_single_chunk_mode1

def create_mock_tfm():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„TimesFMæ¨¡å‹"""
    class MockTFM:
        def forecast_on_df(self, inputs, freq, value_name, num_jobs):
            # åˆ›å»ºæ¨¡æ‹Ÿçš„é¢„æµ‹ç»“æœ
            horizon_len = len(inputs)
            dates = pd.date_range(start='2024-01-01', periods=horizon_len, freq='D')
            
            # ç”Ÿæˆæ¨¡æ‹Ÿçš„åˆ†ä½æ•°é¢„æµ‹
            base_values = np.random.normal(6.5, 0.5, horizon_len)
            
            forecast_data = {
                'ds': dates,
                'timesfm-q-0.1': base_values - 0.4,
                'timesfm-q-0.2': base_values - 0.3,
                'timesfm-q-0.3': base_values - 0.2,
                'timesfm-q-0.4': base_values - 0.1,
                'timesfm-q-0.5': base_values,
                'timesfm-q-0.6': base_values + 0.1,
                'timesfm-q-0.7': base_values + 0.2,
                'timesfm-q-0.8': base_values + 0.3,
                'timesfm-q-0.9': base_values + 0.4,
            }
            
            return pd.DataFrame(forecast_data)
    
    return MockTFM()

def create_mock_chunk():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„æ•°æ®åˆ†å—"""
    horizon_len = 5
    dates = pd.date_range(start='2024-01-01', periods=horizon_len, freq='D')
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„å®é™…è‚¡ä»·æ•°æ®
    actual_prices = [6.8, 6.67, 6.58, 6.61, 6.75]
    
    chunk_data = {
        'ds': dates,
        'close': actual_prices,
        'stock_code': ['600398'] * horizon_len
    }
    
    return pd.DataFrame(chunk_data)

def test_quantile_evaluation():
    """æµ‹è¯•åˆ†ä½æ•°è¯„ä¼°åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•åˆ†ä½æ•°è¯„ä¼°åŠŸèƒ½...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®å’Œæ¨¡å‹
    mock_tfm = create_mock_tfm()
    mock_chunk = create_mock_chunk()
    
    print(f"ğŸ“‹ æµ‹è¯•æ•°æ®:")
    print(f"  åˆ†å—å¤§å°: {len(mock_chunk)}")
    print(f"  å®é™…ä»·æ ¼: {mock_chunk['close'].tolist()}")
    
    try:
        # æ‰§è¡Œé¢„æµ‹
        result = predict_single_chunk_mode1(
            chunk=mock_chunk,
            tfm=mock_tfm,
            chunk_index=0
        )
        
        print(f"\nâœ… é¢„æµ‹æˆåŠŸ!")
        print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"  æœ€ä¼˜åˆ†ä½æ•°: {result.metrics['best_quantile']}")
        print(f"  æœ€ä¼˜ç»¼åˆå¾—åˆ†: {result.metrics['best_combined_score']:.6f}")
        print(f"  æœ€ä¼˜MSE: {result.metrics['mse']:.6f}")
        print(f"  æœ€ä¼˜MAE: {result.metrics['mae']:.6f}")
        
        print(f"\nğŸ“ˆ æ‰€æœ‰åˆ†ä½æ•°è¯„ä¼°:")
        for quantile, metrics in result.metrics['all_quantile_metrics'].items():
            print(f"  {quantile}: MSE={metrics['mse']:.6f}, MAE={metrics['mae']:.6f}, ç»¼åˆå¾—åˆ†={metrics['combined_score']:.6f}")
        
        print(f"\nğŸ¯ é¢„æµ‹å€¼ (æœ€ä¼˜åˆ†ä½æ•° {result.metrics['best_quantile']}):")
        best_predictions = result.predictions[result.metrics['best_quantile']]
        for i, (actual, pred) in enumerate(zip(result.actual_values, best_predictions)):
            print(f"  ç¬¬{i+1}å¤©: å®é™…={actual}, é¢„æµ‹={pred:.4f}, è¯¯å·®={abs(actual-pred):.4f}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_quantile_evaluation()