
from urllib import request
from req_res_types import *
from typing import List, Optional, Dict, Any
import os
import sys
import pandas as pd
import numpy as np
import json
from tqdm import tqdm
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
finance_dir = parent_dir
pre_data_dir = os.path.join(parent_dir, 'preprocess_data')
sys.path.append(pre_data_dir)
ak_tools_dir = os.path.join(parent_dir, 'akshare-tools')
sys.path.append(ak_tools_dir)


def _round_obj(o):
    import numpy as _np
    if isinstance(o, (float, _np.floating)):
        return round(float(o), 4)
    if isinstance(o, list):
        return [_round_obj(v) for v in o]
    if isinstance(o, dict):
        return {k: _round_obj(v) for k, v in o.items()}
    return o
# å¯¼å…¥å…¶ä»–æ¨¡å—
from chunks_functions import create_chunks_from_test_data
from processor import df_preprocess
from math_functions import mean_squared_error, mean_absolute_error
from postgres import PostgresHandler
from timesfm_init import init_timesfm
# åœ¨éœ€è¦æ—¶æ‰å¯¼å…¥timesfm-2.5ç‰ˆæœ¬çš„inferenceæ¨¡å—
def import_predict_2p5():
    # ä¿å­˜åŸå§‹sys.path
    original_sys_path = sys.path.copy()
    
    # æ·»åŠ timesfm-2p5-functionsè·¯å¾„å’Œtimesfm-2.5æºç è·¯å¾„
    timesfm_2P5_dir = os.path.join(current_dir, "timesfm-2p5-functions")
    timesfm_src = os.path.join(timesfm_2P5_dir, "timesfm-2.5", "src")
    
    # åªæ·»åŠ å¿…è¦çš„è·¯å¾„ï¼Œè€Œä¸æ˜¯å®Œå…¨æ›¿æ¢sys.path
    sys.path.insert(0, timesfm_src)
    sys.path.insert(0, timesfm_2P5_dir)
    
    try:
        from inference import predict_2p5
        return predict_2p5
    finally:
        # æ¢å¤åŸå§‹sys.path
        sys.path = original_sys_path

def _parse_unique_key(unique_key: str) -> Optional[Dict[str, Any]]:
    """
    è§£æ unique_keyï¼Œæ ¼å¼çº¦å®šï¼š
    "{symbol}_best_hlen_{horizon_len}_clen_{context_len}_v_{timesfm_version}"

    è¿”å› dictï¼š{
        'symbol': str,
        'horizon_len': int,
        'context_len': int,
        'timesfm_version': str,
    }
    """
    try:
        s = str(unique_key).strip()
        parts = s.split("_best_hlen_")
        if len(parts) != 2:
            return None
        symbol = parts[0]
        rest = parts[1]
        # rest is like: "{hlen}_clen_{clen}_v_{ver}"
        if "_clen_" not in rest:
            return None
        hlen_str, rest2 = rest.split("_clen_", 1)
        if "_v_" not in rest2:
            return None
        clen_str, ver = rest2.split("_v_", 1)
        return {
            "symbol": symbol,
            "horizon_len": int(hlen_str),
            "context_len": int(clen_str),
            "timesfm_version": ver,
        }
    except Exception:
        return None

async def predict_next_chunk_by_unique_key(
        unique_key: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        user_id: Optional[int] = None,
        persist: bool = True,
    ) -> Optional[ChunkPredictionResult]:
    """
    æ ¹æ® unique_key è§£æå‡º symbolã€horizon_lenã€context_lenã€timesfm_versionï¼Œå¹¶é¢„æµ‹â€œä¸‹ä¸€ä¸ªåˆ†å—â€ã€‚

    é€»è¾‘ï¼š
    - è§£æ unique_keyï¼ˆæ ¼å¼ï¼š"{symbol}_best_hlen_{hlen}_clen_{clen}_v_{ver}"ï¼‰
    - ä¾æ® processor.df_preprocess è·å–æ•°æ®é›†ï¼ˆå¯é€‰ start_date/end_date çº¦æŸï¼‰
    - ç”¨æµ‹è¯•é›†æœ€åä¸€ä¸ªåˆ†å—çš„å†å²æ¥æ¨æ–­â€œä¸‹ä¸€åˆ†å—â€çš„è®­ç»ƒçª—å£ï¼ˆè®­ç»ƒ+æµ‹è¯•+éªŒè¯å†å²åˆ°æœ€æ–°ï¼‰
    - åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸º horizon_len çš„æœªæ¥æ—¥æœŸçª—å£ä½œä¸ºâ€œä¸‹ä¸€ä¸ªåˆ†å—â€çš„ df_testï¼ˆè‹¥æ— æ³•ç¡®å®šæ—¥æœŸåˆ™å›é€€åˆ°éªŒè¯é›†æœ«å°¾åçš„é¡ºå»¶ï¼‰
    - ä½¿ç”¨ predict_single_chunk_mode1 åšä¸€æ¬¡é¢„æµ‹
    - è‹¥ persistï¼Œåˆ™è°ƒç”¨ PostgresHandler ä¿å­˜åˆ° /save-predictions/mtf-best/val-chunkï¼ˆæ²¿ç”¨å”¯ä¸€é”®ï¼Œchunk_index ä½¿ç”¨è¿ç»­ä¸‹æ ‡ï¼‰
    """
    try:
        info = _parse_unique_key(unique_key)
        if not info:
            print(f"âŒ æ— æ³•è§£æ unique_key: {unique_key}")
            return None
        symbol = info["symbol"]
        horizon_len = int(info["horizon_len"])
        context_len = int(info["context_len"])
        timesfm_version = str(info["timesfm_version"]).strip()

        # è®¡ç®—ä¸‹ä¸€åˆ†å—çš„ç´¢å¼•ï¼šä¼˜å…ˆä½¿ç”¨åç«¯æœ€æ–°éªŒè¯åˆ†å—çš„ chunk_index+1ï¼›å¦åˆ™åŸºäºæœ¬åœ°æ•°æ®è®¡ç®—
        next_chunk_index = 0
        got_latest_idx = False
        pg_tmp = None
        stock_type = 1
        try:
            base_url = os.environ.get('POSTGRES_API', 'http://go-api.meetlife.com.cn:8000')
            pg_tmp = PostgresHandler(base_url=base_url, api_token="fintrack-dev-token")
            await pg_tmp.open()
            sc_latest, data_latest, _ = await pg_tmp.get_latest_val_chunk(unique_key)
            if sc_latest == 200 and isinstance(data_latest, dict):
                d = data_latest.get('data') if 'data' in data_latest else data_latest
                if isinstance(d, dict):
                    last_start = d.get('start_date')
                    stock_type = d.get('stock_type', 1)
                if last_start:
                    try:
                        last_start_dt = pd.to_datetime(last_start).date()
                        # è‹¥ç”¨æˆ·æœªæ˜¾å¼æŒ‡å®š end_dateï¼Œåˆ™æŒ‰ horizon_len æ¨å¯¼
                        end_date = (pd.Timestamp(last_start_dt)).strftime('%Y-%m-%d')
                        print(f"âœ… åŸºäºæœ€æ–°éªŒè¯åˆ†å—å¼€å§‹æ—¥æœŸ {last_start} æ¨å¯¼endæ—¥æœŸçª—å£ -> {end_date}")
                    except Exception:
                        pass
        except Exception:
            pass
        finally:
            try:
                await pg_tmp.close()
            except Exception:
                pass

        # é¢„å¤„ç†æ•°æ®ï¼ˆåœ¨ç¡®å®š start_date/end_date åè¿›è¡Œï¼‰
        df_original, df_train, df_test, df_val = await df_preprocess(
            stock_code=symbol,
            stock_type=stock_type,
            start_date=None,
            end_date=end_date,
            time_step=0,
            years=15,
            horizon_len=horizon_len,
        )
        if df_original is None:
            print(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {symbol}")
            return None

        # æ„é€ è®­ç»ƒçª—å£ï¼šåœ¨ç¡®å®šäº† start_date/end_date åå†ä½¿ç”¨åˆå§‹è·å–çš„ df_original ä½œä¸ºè®­ç»ƒå†å²
        df_hist = df_original
        try:
            df_hist["unique_id"] = df_hist["stock_code"].astype(str)
        except Exception:
            pass

        # æ„é€ â€œä¸‹ä¸€åˆ†å—â€çš„æ—¥æœŸçª—å£ï¼šä¼˜å…ˆä½¿ç”¨ä¸Šä¸€æ­¥æ¨å¯¼çš„ start_date/end_dateï¼›å¦åˆ™ä»éªŒè¯é›†æœ«å°¾æ—¥æœŸ+1
        try:
            if start_date and end_date:
                future_dates = pd.date_range(start=pd.to_datetime(start_date), end=pd.to_datetime(end_date), freq="D")
                if len(future_dates) > horizon_len:
                    future_dates = future_dates[:horizon_len]
                elif len(future_dates) < horizon_len:
                    # è‹¥ä¸è¶³ï¼Œåˆ™æŒ‰ horizon_len é‡æ–°ç”Ÿæˆ
                    future_dates = pd.date_range(start=pd.to_datetime(start_date), periods=horizon_len, freq="D")
            else:
                last_date = pd.to_datetime(df_hist["ds"].max())
                future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=horizon_len, freq="D")
        except Exception:
            future_dates = pd.date_range(start=pd.to_datetime("today"), periods=horizon_len, freq="D")

        # ä»¥å†å²æœ€åä¸€å¤©ä»·æ ¼ä¸ºåŸºå‡†ï¼Œæ„é€ ä¸€ä¸ªå ä½çš„ df_test ç»“æ„ï¼ˆå®é™…å€¼æœªçŸ¥ï¼Œé¢„æµ‹æ—¶åªéœ€è¦é•¿åº¦ä¸æ—¥æœŸï¼‰
        try:
            base_price = float(df_hist.iloc[-1]["close"]) if "close" in df_hist.columns else None
        except Exception:
            base_price = None
        if base_price is None or base_price <= 0:
            base_price = 1.0

        df_next = pd.DataFrame({
            "ds": future_dates,
            "close": [base_price] * horizon_len,
            "stock_code": [symbol] * horizon_len,
            "unique_id": [symbol] * horizon_len,
        })

        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆ2.0 ç‰ˆæœ¬éœ€è¦ã€2.5 ç”±å†…éƒ¨å‡½æ•°å¤„ç†ï¼‰
        tfm = None
        if timesfm_version == "2.0":
            tfm = init_timesfm(horizon_len=horizon_len, context_len=context_len)

        req = ChunkedPredictionRequest(
            stock_code=symbol,
            years=10,
            horizon_len=horizon_len,
            start_date=start_date,
            end_date=end_date,
            context_len=context_len,
            time_step=0,
            stock_type=1,
            timesfm_version=timesfm_version,
            user_id=user_id,
        )

        result = predict_single_chunk_mode1(
            df_train=df_hist,
            df_test=df_next,
            tfm=tfm,
            chunk_index=next_chunk_index,
            request=req,
        )

        # æŒä¹…åŒ–åˆ°åç«¯ï¼ˆä½œä¸ºæ–°çš„éªŒè¯åˆ†å—ï¼‰
        if persist and result and len(result.actual_values) > 0:
            try:
                base_url = os.environ.get('POSTGRES_API', 'http://go-api.meetlife.com.cn:8000')
                pg = PostgresHandler(base_url=base_url, api_token="fintrack-dev-token")
                await pg.open()

                start_date_str = str(result.chunk_start_date)
                end_date_str = str(result.chunk_end_date)
                size = len(result.actual_values)
                chunk_dates = pd.date_range(
                    start=pd.to_datetime(start_date_str, errors='coerce'),
                    end=pd.to_datetime(end_date_str, errors='coerce'),
                    freq='D'
                )[:size]
                dates_str = [d.strftime('%Y-%m-%d') for d in chunk_dates]

                # ä»…ä¿ç•™ä¸€ä¸ªåˆ†ä½ï¼ˆä¼˜å…ˆ mtf-0.5ï¼‰ä»¥å‡å°‘å­˜å‚¨
                preds_map = result.predictions or {}
                chosen_key = None
                if 'mtf-0.5' in preds_map:
                    chosen_key = 'mtf-0.5'
                else:
                    for k in preds_map.keys():
                        chosen_key = k
                        break
                predictions_clean = {}
                if chosen_key:
                    predictions_clean[chosen_key] = [
                        round(float(x), 4) if x is not None else None for x in preds_map.get(chosen_key, [])
                    ]

                # ç¡®ä¿å­˜åœ¨ timesfm_best_predictions è®°å½•ï¼Œé¿å…å¤–é”®çº¦æŸå¯¼è‡´å†™å…¥å¤±è´¥
                best_confirmed = False
                try:
                    status_code_best, data_best, body_best = await pg.get_best_by_unique(unique_key)
                    best_confirmed = (status_code_best == 200)
                except Exception:
                    best_confirmed = False

                if not best_confirmed:
                    try:
                        def to_date_str(val):
                            try:
                                dt = pd.to_datetime(val, errors='coerce')
                                return dt.strftime('%Y-%m-%d') if not pd.isna(dt) else str(val)
                            except Exception:
                                return str(val)

                        train_start_date = to_date_str(df_train['ds'].min())
                        train_end_date = to_date_str(df_train['ds'].max())
                        test_start_date = to_date_str(df_test['ds'].min())
                        test_end_date = to_date_str(df_test['ds'].max())
                        val_start_date = to_date_str(df_val['ds'].min())
                        val_end_date = to_date_str(df_val['ds'].max())

                        best_metrics_payload = {
                            'best_prediction_item': chosen_key,
                            'source': 'next-chunk-persist'
                        }
                        go_payload = {
                            "unique_key": unique_key,
                            "symbol": symbol,
                            "timesfm_version": timesfm_version,
                            "best_prediction_item": chosen_key or "mtf-0.5",
                            "best_metrics": _round_obj(best_metrics_payload),
                            "train_start_date": train_start_date,
                            "train_end_date": train_end_date,
                            "test_start_date": test_start_date,
                            "test_end_date": test_end_date,
                            "val_start_date": val_start_date,
                            "val_end_date": val_end_date,
                            "context_len": int(context_len),
                            "horizon_len": int(horizon_len),
                            "user_id": user_id,
                            "is_public": 1 if (user_id == 1) else 0,
                        }
                        sc_upsert, data_upsert, body_upsert = await pg.save_best_prediction(go_payload)
                        best_confirmed = (sc_upsert == 200)
                        if best_confirmed:
                            print(f"âœ… å·²è¡¥å†™timesfm-best: unique_key={unique_key}")
                        else:
                            print(f"âš ï¸ è¡¥å†™timesfm-bestå¤±è´¥: status={sc_upsert}, body={body_upsert}")
                    except Exception as add_err:
                        print(f"âš ï¸ å°è¯•è¡¥å†™timesfm-bestå¼‚å¸¸: {add_err}")

                if not best_confirmed:
                    print(f"âš ï¸ è·³è¿‡ä¸‹ä¸€åˆ†å—å†™å…¥ï¼šæœªæ‰¾åˆ°timesfm-best(unique_key={unique_key})ï¼Œé¿å…å¤–é”®å†²çª")
                    try:
                        await pg.close()
                    except Exception:
                        pass
                    return result

                payload = {
                    "unique_key": unique_key,
                    "chunk_index": int(next_chunk_index),
                    "start_date": start_date_str,
                    "end_date": end_date_str,
                    "predictions": predictions_clean,
                    "actual_values": [round(float(x), 4) if x is not None else None for x in result.actual_values],
                    "dates": dates_str,
                    "symbol": symbol,
                    "is_public": 1 if (user_id == 1) else 0,
                    "user_id": user_id,
                }

                status_code, data, body_text = await pg.save_best_val_chunk(_round_obj(payload))
                if status_code == 200:
                    print(f"âœ… ä¸‹ä¸€åˆ†å—å·²ä¿å­˜: unique_key={unique_key}, chunk_index={next_chunk_index}")
                else:
                    print(f"âš ï¸ ä¸‹ä¸€åˆ†å—ä¿å­˜å¤±è´¥: status={status_code}, body={body_text}")
                try:
                    await pg.close()
                except Exception:
                    pass
            except Exception as e:
                print(f"âš ï¸ æŒä¹…åŒ–ä¸‹ä¸€åˆ†å—å¼‚å¸¸: {e}")

        return result
    except Exception as e:
        try:
            print(f"âŒ é¢„æµ‹ä¸‹ä¸€åˆ†å—å¤±è´¥: {e}")
        except Exception:
            pass
        return None

def predict_single_chunk_mode1(
        df_train: pd.DataFrame,
        df_test: pd.DataFrame, 
        tfm, 
        chunk_index: int,
        request: ChunkedPredictionRequest,
    ) -> ChunkPredictionResult:
    """
    æ¨¡å¼1ï¼šå¯¹å•ä¸ªåˆ†å—è¿›è¡Œé¢„æµ‹ï¼ˆå›ºå®šè®­ç»ƒé›†ï¼Œä½¿ç”¨ak_stock_dataç”Ÿæˆæµ‹è¯•æ•°æ®ï¼‰
    
    Args:
        df_train: å›ºå®šçš„è®­ç»ƒæ•°æ®
        df_test: å½“å‰åˆ†å—çš„æµ‹è¯•æ•°æ®
        tfm: TimesFMæ¨¡å‹å®ä¾‹
        stock_code: è‚¡ç¥¨ä»£ç 
        chunk_index: åˆ†å—ç´¢å¼•
        
    Returns:
        ChunkPredictionResult: åˆ†å—é¢„æµ‹ç»“æœ
    """
    try:
        if request.timesfm_version == "2.0":
            # ä½¿ç”¨æ–°æ•°æ®é›†è¿›è¡Œé¢„æµ‹
            # print(f"æ­£åœ¨ä½¿ç”¨TimesFM-2.0æ¨¡å‹å¯¹æµ‹è¯•é›†åˆ†å— {chunk_index} è¿›è¡Œé¢„æµ‹...")
            forecast_df = tfm.forecast_on_df(
                inputs=df_train,
                freq="D",
                value_name="close",
                num_jobs=1,
            )
            rename_dict = {c: f"mtf-{c.split('timesfm-q-')[1]}" for c in forecast_df.columns if c.startswith('timesfm-q-')}
            rename_dict["timesfm"] = "mtf"
            if rename_dict:
                forecast_df = forecast_df.rename(columns=rename_dict)
        elif request.timesfm_version == "2.5":
            # print(f"æ­£åœ¨ä½¿ç”¨TimesFM-2.5æ¨¡å‹å¯¹æµ‹è¯•é›†åˆ†å— {chunk_index} è¿›è¡Œé¢„æµ‹...")
            predict_2p5_func = import_predict_2p5()
            forecast_df = predict_2p5_func(df_train, max_context=request.context_len, pred_horizon=request.horizon_len, unique_id=request.stock_code)

        df_train_last_one = df_train.iloc[-1]
        # è·å–é¢„æµ‹ç»“æœçš„å‰horizon_lenæ¡è®°å½•
        horizon_len = len(df_test)
        forecast_chunk = forecast_df.head(horizon_len)
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°é¢„æµ‹ç»“æœçš„åˆ—å
        # print(f"  é¢„æµ‹ç»“æœåˆ—å: {list(forecast_df.columns)}")
        # print(f"  é¢„æµ‹ç»“æœå½¢çŠ¶: {forecast_df.shape}")
        # print(f"  é¢„æµ‹ç»“æœå‰{horizon_len}è¡Œ: ")
        # print(forecast_chunk.head(horizon_len))
        # print(f"  æµ‹è¯•æ•°æ®å‰{horizon_len}è¡Œ: ")
        # print(df_test.head(horizon_len))

        # æå–é¢„æµ‹å€¼å’Œå®é™…å€¼
        actual_values = df_test['close'].tolist()
        actual_dates = df_test['ds'].tolist()
        # print(f"  å®é™…æ—¥æœŸå‰7è¡Œ: {actual_dates[:7]}")
        # è·å–æ‰€æœ‰é¢„æµ‹åˆ†ä½æ•°
        predictions = {}
        forecast_columns = [col for col in forecast_chunk.columns if col.startswith('mtf-')]
        
        # print(f"æ‰¾åˆ°çš„é¢„æµ‹åˆ—: {forecast_columns}")
        
        for col in forecast_columns:
            predictions[col] = forecast_chunk[col].tolist()
        
        # è®¡ç®—æ‰€æœ‰åˆ†ä½æ•°çš„è¯„ä¼°æŒ‡æ ‡
        quantile_metrics = {}
        best_quantile_colname = None
        best_quantile_colname_pct = None
        best_score = float('inf')
        best_diff_pct = float('inf') # æœ€ä¼˜æ¶¨è·Œå¹…ç™¾åˆ†æ¯”å·®
        # å®šä¹‰è¦è¯„ä¼°çš„åˆ†ä½æ•°èŒƒå›´ (0.1 åˆ° 0.9)
        target_quantiles = [f'mtf-0.{i}' for i in range(1, 10)]
        
        for quantile in target_quantiles:
            if quantile in predictions:
                pred_values = predictions[quantile]
                
                # ç¡®ä¿é¢„æµ‹å€¼å’Œå®é™…å€¼é•¿åº¦ä¸€è‡´
                min_len = min(len(pred_values), len(actual_values))
                pred_values_trimmed = pred_values[:min_len]
                actual_values_trimmed = actual_values[:min_len]
                # ç¡®ä¿é¢„æµ‹å€¼å’Œå®é™…å€¼é•¿åº¦ä¸€è‡´
                base_price = float(df_train_last_one['close']) if 'close' in df_train_last_one else actual_values_trimmed[0]
                if not base_price or base_price == 0:
                    base_price = actual_values_trimmed[0]
                # è®¡ç®—MSEå’ŒMAE
                mse_q = mean_squared_error(np.array(pred_values_trimmed), np.array(actual_values_trimmed))
                mae_q = mean_absolute_error(np.array(pred_values_trimmed), np.array(actual_values_trimmed))
                pct_q = (pred_values_trimmed[-1] / base_price - 1) * 100
                actual_pct = (actual_values_trimmed[-1] / base_price - 1) * 100
                if actual_pct > 0:
                    diff_pct = abs(pct_q - actual_pct) / actual_pct # é¢„æµ‹æ¶¨è·Œå¹…ä¸å®é™…æ¶¨è·Œå¹…çš„ç™¾åˆ†æ¯”å·®
                else:
                    diff_pct = 1 # å®é™…æ¶¨è·Œå¹…ä¸º0æ—¶ï¼Œè®¾ç½®ä¸ºæ— ç©·å¤§
                # è®¡ç®—ç»¼åˆå¾—åˆ† (MSEå’ŒMAEå„å 50%æƒé‡)
                # ä¸ºäº†ç»Ÿä¸€é‡çº²ï¼Œå¯¹MSEå’ŒMAEè¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
                combined_score = 0.5 * mse_q + 0.5 * mae_q

                # è®¡ç®—è¯¥åˆ†ä½æ•°çš„MLEä¸å¹³å‡è´Ÿå¯¹æ•°ä¼¼ç„¶
                try:
                    # è®¡ç®—æ®‹å·®ï¼šå®é™…å€¼å‡å»é¢„æµ‹å€¼
                    residuals_q = np.array(actual_values_trimmed, dtype=float) - np.array(pred_values_trimmed, dtype=float)
                    # ç”¨æ®‹å·®çš„æ ‡å‡†å·®ä¼°è®¡å™ªå£°æ ‡å‡†å·® ÏƒÌ‚
                    sigma_hat_q = float(np.sqrt(np.mean(residuals_q ** 2)))
                    # è‹¥ ÏƒÌ‚ â‰¤ 0ï¼Œåˆ™åŠ ä¸€ä¸ªæå°å€¼é˜²æ­¢é™¤é›¶ï¼›å¦åˆ™ä¸åŠ 
                    eps_q = 1e-8 if sigma_hat_q <= 0 else 0.0
                    sigma_eff_q = sigma_hat_q + eps_q
                    # è®¡ç®—å¹³å‡è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰ï¼Œå‡è®¾æ®‹å·®æœä» N(0, ÏƒÂ²)
                    avg_nll_q = float(0.5 * np.mean(np.log(2 * np.pi * (sigma_eff_q ** 2)) + (residuals_q ** 2) / (sigma_eff_q ** 2)))
                except Exception as e:
                    # è‹¥è®¡ç®—å¤±è´¥ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯å¹¶ç½®ç©º
                    print(f"  è®¡ç®—åˆ†ä½æ•° {quantile} çš„MLEå’Œå¹³å‡è´Ÿå¯¹æ•°ä¼¼ç„¶æ—¶å‡ºé”™: {str(e)} ç¬¬{e.__traceback__.tb_lineno}è¡Œ")
                    sigma_hat_q = None
                    avg_nll_q = None
                
                quantile_metrics[quantile] = {
                    'mse': mse_q,
                    'mae': mae_q,
                    'combined_score': combined_score,
                    'pred_pct': pct_q,
                    'actual_pct': actual_pct,
                    'diff_pct': diff_pct,
                    'pred_values': pred_values_trimmed,
                    'actual_values': actual_values_trimmed,
                    'mle': sigma_hat_q,
                    'avg_nll': avg_nll_q
                }
                
                # æ‰¾åˆ°æœ€ä¼˜åˆ†ä½æ•°
                if combined_score < best_score:
                    best_score = combined_score
                    best_quantile_colname = quantile
                if diff_pct < best_diff_pct:
                    best_diff_pct = diff_pct
                    best_quantile_colname_pct = quantile
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„åˆ†ä½æ•°é¢„æµ‹ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not quantile_metrics:
            print(f"  âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„åˆ†ä½æ•°é¢„æµ‹ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            mse = 0.0
            mae = 0.0
            best_quantile_colname = 'mtf-0.5'
        else:
            # ä½¿ç”¨æœ€ä¼˜åˆ†ä½æ•°çš„æŒ‡æ ‡
            mse = quantile_metrics[best_quantile_colname]['mse']
            mae = quantile_metrics[best_quantile_colname]['mae']
            
            # print(f"  ğŸ“Š åˆ†ä½æ•°è¯„ä¼°ç»“æœ:")
            # for q, metrics in quantile_metrics.items():
            #     print(f"    {q}: MSE={metrics['mse']:.2f}, MAE={metrics['mae']:.2f}, ç»¼åˆå¾—åˆ†={metrics['combined_score']:.2f}, é¢„æµ‹æ¶¨è·Œå¹…={metrics['pred_pct']:.2f}, å®é™…æ¶¨è·Œå¹…={metrics['actual_pct']:.2f}, ç™¾åˆ†æ¯”å·®={metrics['diff_pct']:.2f}")
            # print(f"  ğŸ† æœ€ä¼˜åˆ†ä½æ•°: {best_quantile_colname} (ç»¼åˆå¾—åˆ†: {best_score:.6f})")
            # print(f"  ğŸ† æœ€ä¼˜åˆ†ä½æ•°(æ¶¨è·Œå¹…): {best_quantile_colname_pct} (ç™¾åˆ†æ¯”å·®: {best_diff_pct:.2f})")
            # print(f"  æœ€ä¼˜(æ¶¨è·Œå¹…)é¢„æµ‹å€¼: {quantile_metrics[best_quantile_colname_pct]['pred_values']}")
            # print(f"  æœ€ä¼˜(æ¶¨è·Œå¹…)å®é™…å€¼: {quantile_metrics[best_quantile_colname_pct]['actual_values']}")
            forecast_chunk["best_quantile_colname_pct"] = best_quantile_colname_pct
            forecast_chunk["best_quantile_colname"] = best_quantile_colname
            forecast_chunk["best_diff_pct"] = best_diff_pct
            forecast_chunk["best_score"] = best_score
            forecast_chunk["best_pred_pct"] = quantile_metrics[best_quantile_colname_pct]['pred_pct']
            forecast_chunk["actual_pct"] = quantile_metrics[best_quantile_colname_pct]['actual_pct']
            forecast_chunk["diff_pct"] = quantile_metrics[best_quantile_colname_pct]['diff_pct']
            forecast_chunk["mse"] = quantile_metrics[best_quantile_colname_pct]['mse']
            forecast_chunk["mae"] = quantile_metrics[best_quantile_colname_pct]['mae']
            forecast_chunk["combined_score"] = quantile_metrics[best_quantile_colname_pct]['combined_score']
            forecast_chunk["symbol"] = forecast_chunk["unique_id"]
            # try:
            #     payload = []
            #     for _, row in forecast_chunk.iterrows():
            #         item = {
            #             "symbol": row.get("symbol"),
            #             "ds": str(row.get("ds")),
            #             "mtf": float(row.get("mtf")) if row.get("mtf") is not None else 0.0,
            #             "mtf_01": float(row.get("mtf-0.1")) if row.get("mtf-0.1") is not None else 0.0,
            #             "mtf_02": float(row.get("mtf-0.2")) if row.get("mtf-0.2") is not None else 0.0,
            #             "mtf_03": float(row.get("mtf-0.3")) if row.get("mtf-0.3") is not None else 0.0,
            #             "mtf_04": float(row.get("mtf-0.4")) if row.get("mtf-0.4") is not None else 0.0,
            #             "mtf_05": float(row.get("mtf-0.5")) if row.get("mtf-0.5") is not None else 0.0,
            #             "mtf_06": float(row.get("mtf-0.6")) if row.get("mtf-0.6") is not None else 0.0,
            #             "mtf_07": float(row.get("mtf-0.7")) if row.get("mtf-0.7") is not None else 0.0,
            #             "mtf_08": float(row.get("mtf-0.8")) if row.get("mtf-0.8") is not None else 0.0,
            #             "mtf_09": float(row.get("mtf-0.9")) if row.get("mtf-0.9") is not None else 0.0,
            #             "chunk_index": chunk_index,
            #             "best_quantile": str(best_quantile_colname),
            #             "best_quantile_pct": str(best_quantile_colname_pct),
            #             "best_pred_pct": float(quantile_metrics[best_quantile_colname_pct]['pred_pct']),
            #             "actual_pct": float(quantile_metrics[best_quantile_colname_pct]['actual_pct']),
            #             "diff_pct": float(quantile_metrics[best_quantile_colname_pct]['diff_pct']),
            #             "mse": float(quantile_metrics[best_quantile_colname_pct]['mse']),
            #             "mae": float(quantile_metrics[best_quantile_colname_pct]['mae']),
            #             "combined_score": float(quantile_metrics[best_quantile_colname_pct]['combined_score']),
            #         }
            #         payload.append(item)

            #     import requests
            #     base_url = os.environ.get("GO_API_BASE_URL", "http://localhost:8080")
            #     token = os.environ.get("API_TOKEN", "fintrack-dev-token")
            #     url = f"{base_url.rstrip('/')}/api/v1/timesfm/forecast/batch"
            #     headers = {"Content-Type": "application/json", "X-Token": token}
            #     resp = requests.post(url, json=payload, headers=headers, timeout=3)
            #     if resp.status_code != 200:
            #         print(f"âš ï¸ å†™å…¥PGå¤±è´¥: HTTP {resp.status_code} {resp.text[:256]}")
            #     else:
            #         print(f"âœ… å·²å†™å…¥PGé¢„æµ‹ç»“æœ: {len(payload)} æ¡, chunk={chunk_index}")
            # except Exception as e:
            #     print(f"âš ï¸ å†™å…¥PGå¼‚å¸¸: {e}")
        # è·å–å®é™…å€¼å’Œé¢„æµ‹å€¼å¯¹åº”çš„æ—¥æœŸèŒƒå›´
        # å®é™…å€¼å’Œé¢„æµ‹å€¼å¯¹åº”çš„æ˜¯åˆ†å—ä¸­çš„æœ€åhorizon_lenä¸ªæ—¥æœŸ
        chunk_dates = df_test['ds'].tolist()
        prediction_start_date = chunk_dates[-len(actual_values)].strftime('%Y-%m-%d') if len(actual_values) > 0 else chunk['ds'].min().strftime('%Y-%m-%d')
        prediction_end_date = chunk_dates[-1].strftime('%Y-%m-%d')
        
        # ä¿æŒåŸæœ‰çš„åˆ†å—æ—¥æœŸèŒƒå›´ä½œä¸ºå¤‡ç”¨
        chunk_start_date = prediction_start_date
        chunk_end_date = prediction_end_date

        
        return ChunkPredictionResult(
            chunk_index=chunk_index,
            chunk_start_date=chunk_start_date,
            chunk_end_date=chunk_end_date,
            predictions=predictions,
            actual_values=actual_values,
            metrics={
                'mse': mse, 
                'mae': mae,
                'mle': avg_nll_q if avg_nll_q is not None else float('inf'),
                'best_quantile_colname': best_quantile_colname,
                'best_quantile_colname_pct': best_quantile_colname_pct,
                'best_combined_score': best_score,
                'best_diff_pct': best_diff_pct,
                'all_quantile_metrics': quantile_metrics,

            }
        )
        
    except Exception as e:
        print(f"åˆ†å— {chunk_index} é¢„æµ‹å¤±è´¥: {str(e)} ç¬¬{e.__traceback__.tb_lineno}è¡Œ")
        # è¿”å›ç©ºç»“æœ
        return ChunkPredictionResult(
            chunk_index=chunk_index,
            chunk_start_date="",
            chunk_end_date="",
            predictions={},
            actual_values=[],
            metrics={
                'mse': float('inf'), 
                'mae': float('inf'),
                'best_quantile_colname': 'mtf',
                'best_quantile_colname_pct': 'mtf',
                'best_combined_score': float('inf'),
                'best_diff_pct': float('inf'),
                'all_quantile_metrics': {}
            }
        )

async def predict_chunked_mode_for_best(request: ChunkedPredictionRequest) -> ChunkedPredictionResponse:
    """
    æ¨¡å¼1åˆ†å—é¢„æµ‹ä¸»å‡½æ•° - æ”¯æŒåˆ†å—é¢„æµ‹ã€æœ€ä½³åˆ†æ•°é€‰æ‹©å’Œåœ¨éªŒè¯é›†ä¸ŠéªŒè¯
    
    Args:
        request: åˆ†å—é¢„æµ‹è¯·æ±‚
        tfm: TimesFMæ¨¡å‹å®ä¾‹
        
    Returns:
        ChunkedPredictionResponse: åˆ†å—é¢„æµ‹å“åº”ï¼ŒåŒ…å«æœ€ä½³é¢„æµ‹é¡¹å’ŒéªŒè¯ç»“æœ
    """
    import time
    start_time = time.time()
    try:
        # æ•°æ®é¢„å¤„ç†
        df_original, df_train, df_test, df_val = await df_preprocess(
            request.stock_code, 
            request.stock_type, 
            request.start_date,
            request.end_date,
            request.time_step, 
            years=request.years, 
            horizon_len=request.horizon_len
        )
        
        # æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ˜¯å¦æˆåŠŸ
        if df_original is None or df_train is None or df_test is None or df_val is None:
            print(f"âŒ è‚¡ç¥¨ {request.stock_code} æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹")
            return ChunkedPredictionResponse(
                stock_code=request.stock_code,
                total_chunks=0,
                horizon_len=request.horizon_len,
                context_len=request.context_len,
                chunk_results=[],
                overall_metrics={
                    'avg_mse': float('inf'),
                    'avg_mae': float('inf'),
                    'error': 'Data preprocessing failed'
                },
                processing_time=time.time() - start_time
            )
        
        print(f"âœ… è‚¡ç¥¨ {request.stock_code} æ•°æ®é¢„å¤„ç†æˆåŠŸ")
        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: è®­ç»ƒé›†={len(df_train)}, æµ‹è¯•é›†={len(df_test)}, éªŒè¯é›†={len(df_val)}")
        
        # æ·»åŠ å”¯ä¸€æ ‡è¯†ç¬¦
        df_train["unique_id"] = df_train["stock_code"].astype(str)
        df_test["unique_id"] = df_test["stock_code"].astype(str)
        df_val["unique_id"] = df_val["stock_code"].astype(str)
        
        # å¯¹æµ‹è¯•æ•°æ®è¿›è¡Œåˆ†å—ï¼ˆè‡ªåŠ¨è®¡ç®—åˆ†å—æ•°é‡ï¼Œä¸ä½¿ç”¨chunk_numé™åˆ¶ï¼‰
        chunks = create_chunks_from_test_data(df_test, request.horizon_len)
        active_chunks = chunks
        
        # å¯¹æ¯ä¸ªåˆ†å—è¿›è¡Œé¢„æµ‹
        chunk_results = []
        all_mse = []
        all_mae = []
        all_predictions = []  # å­˜å‚¨æ‰€æœ‰åˆ†å—çš„æ‰€æœ‰é¢„æµ‹ç»“æœ
        
        if len(active_chunks) == 0:
            print(f"âŒ è‚¡ç¥¨ {request.stock_code} æµ‹è¯•é›†åˆ†å—ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹")
            return ChunkedPredictionResponse(
                stock_code=request.stock_code,
                total_chunks=0,
                horizon_len=request.horizon_len,
                chunk_results=[],
                overall_metrics={
                    'avg_mse': float('inf'),
                    'avg_mae': float('inf'),
                    'error': 'Empty test chunks'
                },
                processing_time=time.time() - start_time
            )
        if request.timesfm_version == "2.5":
            tfm = None
        if request.timesfm_version == "2.0":
            tfm = init_timesfm(request.horizon_len, request.context_len)
        tqdm_bar = tqdm(total=len(active_chunks), desc="å¤„ç†æµ‹è¯•é›†åˆ†å—")
        for i, chunk in enumerate(active_chunks):
            tqdm_bar.update(1)
            tqdm_bar.set_description(f"å¤„ç†æµ‹è¯•é›†åˆ†å— {i+1}/{len(active_chunks)}")
            tqdm_bar.refresh()
            history_len = i * request.horizon_len
            if history_len > 0:
                df_train_current = pd.concat([df_train, df_test.iloc[:history_len, :]], axis=0)
            else:
                df_train_current = df_train
            df_train_last_one = df_train_current.iloc[-1, :]
            result = predict_single_chunk_mode1(
                df_train=df_train_current,
                df_test=chunk,
                tfm=tfm,
                chunk_index=i,
                request=request,
            )
            
            chunk_results.append(result)
            
            # æ”¶é›†æŒ‡æ ‡ç”¨äºè®¡ç®—æ€»ä½“æŒ‡æ ‡
            if result.metrics['mse'] != float('inf'):
                all_mse.append(result.metrics['mse'])
                all_mae.append(result.metrics['mae'])
                
            # æ”¶é›†æ‰€æœ‰é¢„æµ‹ç»“æœ
            if result.predictions:
                all_predictions.append({
                    'chunk_index': i,
                    'predictions': result.predictions,
                    'actual_values': result.actual_values,
                    'dates': pd.date_range(
                        start=pd.to_datetime(result.chunk_start_date),
                        end=pd.to_datetime(result.chunk_end_date),
                        freq='D'
                    )[:len(result.actual_values)]
                })
        
        # åˆ†ææœ€ä½³é¢„æµ‹é¡¹ (mtf-0.1 åˆ° mtf-0.9)
        best_prediction_item = None
        best_score = float('inf')
        best_metrics = {}
        
        prediction_items = [f"mtf-0.{i}" for i in range(1, 10)]
        
        for item in prediction_items:
            item_mse = []
            item_mae = []
            item_returns = []  # æ¶¨è·Œå¹…
            item_mle = []
            
            for pred_data in all_predictions:
                if item in pred_data['predictions']:
                    pred_values = pred_data['predictions'][item]
                    actual_values = pred_data['actual_values']
                    
                    # è®¡ç®—MSEå’ŒMAE
                    mse = mean_squared_error(actual_values, pred_values)
                    mae = mean_absolute_error(actual_values, pred_values)
                    item_mse.append(mse)
                    item_mae.append(mae)
                    
                    # è®¡ç®—æ¶¨è·Œå¹…ï¼šç»Ÿä¸€ä»¥ df_train_last_one çš„æ”¶ç›˜ä»·ä¸ºèµ·ç‚¹
                    if len(pred_values) >= 1 and len(actual_values) >= 1:
                        base_price = float(df_train_last_one['close']) if 'close' in df_train_last_one else actual_values[0]
                        if not base_price or base_price == 0:
                            base_price = actual_values[0]
                        pred_return = (pred_values[-1] - base_price) / base_price * 100
                        actual_return = (actual_values[-1] - base_price) / base_price * 100
                        item_returns.append(abs(pred_return - actual_return))

                    try:
                        chunk_idx = pred_data['chunk_index']
                        cr = chunk_results[chunk_idx]
                        qm = (cr.metrics or {}).get('all_quantile_metrics', {})
                        mle_val = None
                        if item in qm:
                            mle_val = qm[item].get('mle')
                        if mle_val is None:
                            min_len_q = min(len(pred_values), len(actual_values))
                            if min_len_q > 0:
                                residuals_q = np.array(actual_values[:min_len_q], dtype=float) - np.array(pred_values[:min_len_q], dtype=float)
                                mle_val = float(np.sqrt(np.mean(residuals_q ** 2)))
                        if mle_val is not None:
                            item_mle.append(mle_val)
                    except Exception:
                        pass
            
            if item_mse:
                avg_mse = np.mean(item_mse)
                avg_mae = np.mean(item_mae)
                avg_return_diff = np.var(item_returns) if item_returns else float('inf')
                avg_mle = np.var(item_mle) if item_mle else float('inf')
                # ç»¼åˆè¯„åˆ† (MSEæƒé‡0.3, MAEæƒé‡0.3, æ¶¨è·Œå¹…å·®å¼‚æƒé‡0.4)
                # composite_score = 0.3 * avg_mse + 0.3 * avg_mae + 0.4 * avg_return_diff
                composite_score = 0.3 * avg_mse + 0.3 * avg_mae + 0.4 * avg_return_diff
                
                if composite_score < best_score:
                    best_score = composite_score
                    best_prediction_item = item
                    best_metrics = {
                        'mse': avg_mse,
                        'mae': avg_mae,
                        'return_diff': avg_return_diff,
                        'mle': avg_mle,
                        'composite_score': composite_score
                    }
        
        print(f"ğŸ¯ æœ€ä½³é¢„æµ‹é¡¹: {best_prediction_item}")
        print(f"ğŸ“Š æœ€ä½³æŒ‡æ ‡: MSE={best_metrics.get('mse', 'N/A'):.4f}, "
                f"MAE={best_metrics.get('mae', 'N/A'):.4f}, "
                f"æ¶¨è·Œå¹…å·®å¼‚={best_metrics.get('return_diff', 'N/A'):.2f}%, "
                f"MLE={best_metrics.get('mle', 'N/A'):.4f}, "
                f"ç»¼åˆè¯„åˆ†={best_metrics.get('composite_score', 'N/A'):.4f}")
        
        # åœ¨éªŒè¯é›†ä¸Šä½¿ç”¨æœ€ä½³é¢„æµ‹é¡¹è¿›è¡ŒéªŒè¯
        saved_best_ok = False
        validation_results = None
        val_results: List[ChunkPredictionResult] = []
        if best_prediction_item and len(df_val) >= request.horizon_len:
            print(f"ğŸ” ä½¿ç”¨æœ€ä½³é¢„æµ‹é¡¹ {best_prediction_item} åœ¨éªŒè¯é›†ä¸Šè¿›è¡ŒéªŒè¯...")
            val_resp = await predict_validation_chunks_only(
                request,
                tfm=tfm,
                timesfm_version=request.timesfm_version,
                fixed_best_prediction_item=best_prediction_item,
                persist_best=False,
                persist_val_chunks=True,
            )
            val_results = val_resp.validation_chunk_results or []
            try:
                vr = val_resp.overall_metrics.get('validation_results') if isinstance(val_resp.overall_metrics, dict) else None
            except Exception:
                vr = None
            validation_results = vr
            if validation_results:
                print(
                    f"âœ… éªŒè¯ç»“æœ: MSE={validation_results.get('validation_mse', float('inf')):.4f}, "
                    f"MAE={validation_results.get('validation_mae', float('inf')):.4f}, "
                    f"MLE={validation_results.get('validation_mle', float('inf')):.4f}, "
                    f"æ¶¨è·Œå¹…å·®å¼‚={validation_results.get('validation_return_diff', float('inf')):.2f}%"
                )
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        overall_metrics = {
            'avg_mse': np.mean(all_mse) if all_mse else float('inf'),
            'avg_mae': np.mean(all_mae) if all_mae else float('inf'),
            'total_chunks': len(chunks),
            'successful_chunks': len(all_mse),
            'best_prediction_item': best_prediction_item,
            'best_metrics': best_metrics,
            'validation_results': validation_results
        }

        base_url = os.environ.get('POSTGRES_API', 'http://go-api.meetlife.com.cn:8000')
        pg = PostgresHandler(base_url=base_url, api_token="fintrack-dev-token")
        await pg.open()

        # å°†æœ€ä½³åˆ†ä½æ•°æŒ‰è‚¡ç¥¨ä»£ç å†™å…¥ JSONï¼Œä¾¿äºå›æµ‹ç›´æ¥è¯»å–
        try:
            out_dir = os.path.join(finance_dir, "forecast-results")
            os.makedirs(out_dir, exist_ok=True)
            out_path = os.path.join(out_dir, f"{request.stock_code}_best_hlen_{request.horizon_len}_clen_{request.context_len}_v_{request.timesfm_version}.json")
            payload = {
                "stock_code": request.stock_code,
                "best_prediction_item": best_prediction_item,
                "timesfm_version": request.timesfm_version,
                "best_metrics": _round_obj(best_metrics),
            }
            with open(out_path, "w", encoding="utf-8") as f:
                json.dump(_round_obj(payload), f, ensure_ascii=False, indent=2)
            print(f"âœ… æœ€ä½³åˆ†ä½æ•°å·²ä¿å­˜: {out_path} -> {best_prediction_item}")

            try:
                def to_date_str(x):
                    try:
                        return pd.to_datetime(x).strftime('%Y-%m-%d')
                    except Exception:
                        return str(x)

                train_start_date = to_date_str(df_train['ds'].min())
                train_end_date = to_date_str(df_train['ds'].max())
                test_start_date = to_date_str(df_test['ds'].min())
                test_end_date = to_date_str(df_test['ds'].max())
                val_start_date = to_date_str(df_val['ds'].min())
                val_end_date = to_date_str(df_val['ds'].max())

                unique_key = f"{request.stock_code}_best_hlen_{request.horizon_len}_clen_{request.context_len}_v_{request.timesfm_version}"

                go_payload = {
                    "unique_key": unique_key,
                    "symbol": request.stock_code,
                    "timesfm_version": request.timesfm_version,
                    "best_prediction_item": best_prediction_item,
                    "best_metrics": _round_obj(best_metrics),
                    "train_start_date": train_start_date,
                    "train_end_date": train_end_date,
                    "test_start_date": test_start_date,
                    "test_end_date": test_end_date,
                    "val_start_date": val_start_date,
                    "val_end_date": val_end_date,
                    "context_len": int(request.context_len),
                    "horizon_len": int(request.horizon_len),
                    "user_id": request.user_id,
                    "is_public": 1 if request.user_id == 1 else 0,
                    "stock_type": int(request.stock_type),
                }

                status_code, data, body_text = await pg.save_best_prediction(go_payload)
                if status_code == 200:
                    print(f"âœ… å·²é€šè¿‡Goåç«¯ä¿å­˜åˆ°PG: unique_key={unique_key}")
                    saved_best_ok = True
                else:
                    print(f"âš ï¸ Goåç«¯ä¿å­˜å¤±è´¥: status={status_code}, body={body_text}")
            except Exception as go_err:
                print(f"âš ï¸ è°ƒç”¨Goåç«¯ä¿å­˜åˆ°PGå¤±è´¥: {go_err}")
        except Exception as save_err:
            print(f"âš ï¸ ä¿å­˜æœ€ä½³åˆ†ä½ JSON å¤±è´¥: {save_err}")

        # éªŒè¯åˆ†å—çš„æŒä¹…åŒ–ç”± predict_validation_chunks_only å¤„ç†ï¼›æ­¤å¤„ä»…åœ¨æœªä¿å­˜bestæ—¶æç¤º
        if val_results and not saved_best_ok:
            print("âš ï¸ è·³è¿‡éªŒè¯åˆ†å—å†™å…¥ï¼šæœªæˆåŠŸä¿å­˜timesfm-bestï¼Œé¿å…å¤–é”®å†²çª")

        try:
            await pg.close()
        except Exception:
            pass
        
        # æ‹¼æ¥æ‰€æœ‰åˆ†å—çš„é¢„æµ‹ç»“æœ
        concatenated_predictions = {}
        concatenated_actual = []
        concatenated_dates = []
        
        if chunk_results:
            # è·å–é¢„æµ‹åˆ—åï¼ˆä»ç¬¬ä¸€ä¸ªåˆ†å—ç»“æœä¸­è·å–ï¼‰
            prediction_columns = list(chunk_results[0].predictions.keys())
            
            # åˆå§‹åŒ–æ‹¼æ¥é¢„æµ‹ç»“æœå­—å…¸
            for col in prediction_columns:
                concatenated_predictions[col] = []
            
            # æ‹¼æ¥æ¯ä¸ªåˆ†å—çš„ç»“æœ
            for result in chunk_results:
                start_date = pd.to_datetime(result.chunk_start_date, errors='coerce')
                end_date = pd.to_datetime(result.chunk_end_date, errors='coerce')
                chunk_size = len(result.actual_values)
                if chunk_size == 0 or pd.isna(start_date) or pd.isna(end_date):
                    continue

                for col in prediction_columns:
                    if col in result.predictions:
                        concatenated_predictions[col].extend(result.predictions[col])
                    else:
                        concatenated_predictions[col].extend([float('nan')] * chunk_size)

                concatenated_actual.extend(result.actual_values)

                chunk_dates = pd.date_range(start=start_date, end=end_date, freq='D')
                concatenated_dates.extend([date.strftime('%Y-%m-%d') for date in chunk_dates[:chunk_size]])
        
        processing_time = time.time() - start_time
        
        resp = ChunkedPredictionResponse(
            stock_code=request.stock_code,
            total_chunks=len(chunks),
            horizon_len=request.horizon_len,
            context_len=request.context_len,
            chunk_results=chunk_results,
            overall_metrics=overall_metrics,
            processing_time=processing_time,
            concatenated_predictions=concatenated_predictions if concatenated_predictions else None,
            concatenated_actual=concatenated_actual if concatenated_actual else None,
            concatenated_dates=concatenated_dates if concatenated_dates else None,
            validation_chunk_results=val_results if val_results else None
        )

        # å°†å®Œæ•´çš„åˆ†å—å“åº”ä¿å­˜ä¸º JSONï¼Œä¾¿äºåç»­ç›´æ¥åŠ è½½å¹¶è·³è¿‡é¢„æµ‹
        try:
            out_dir = os.path.join(finance_dir, "forecast-results")
            os.makedirs(out_dir, exist_ok=True)
            out_path = os.path.join(out_dir, f"{request.stock_code}_chunked_response.json")

            def _cr_to_dict(cr: ChunkPredictionResult):
                return {
                    "chunk_index": cr.chunk_index,
                    "chunk_start_date": cr.chunk_start_date,
                    "chunk_end_date": cr.chunk_end_date,
                    "predictions": cr.predictions,
                    "actual_values": cr.actual_values,
                    "metrics": cr.metrics,
                }

            payload = {
                "stock_code": resp.stock_code,
                "total_chunks": resp.total_chunks,
                "horizon_len": resp.horizon_len,
                "chunk_results": [ _cr_to_dict(cr) for cr in (resp.chunk_results or []) ],
                "overall_metrics": resp.overall_metrics,
                "processing_time": resp.processing_time,
                "concatenated_predictions": resp.concatenated_predictions,
                "concatenated_actual": resp.concatenated_actual,
                "concatenated_dates": resp.concatenated_dates,
                "validation_chunk_results": [ _cr_to_dict(vcr) for vcr in (resp.validation_chunk_results or []) ] if resp.validation_chunk_results else None,
            }

            with open(out_path, "w", encoding="utf-8") as f:
                json.dump(_round_obj(payload), f, ensure_ascii=False, indent=2)
            print(f"âœ… åˆ†å—å“åº”å·²ä¿å­˜: {out_path}")
        except Exception as save_err:
            print(f"âš ï¸ ä¿å­˜åˆ†å—å“åº” JSON å¤±è´¥: {save_err}")

        return resp
    except Exception as e:
        # å…œåº•ï¼šä¸»æµç¨‹å¼‚å¸¸æ—¶è¿”å›å ä½å“åº”ï¼Œå¹¶æ‰“å°é”™è¯¯ä¿¡æ¯
        processing_time = time.time() - start_time
        try:
            lineno = e.__traceback__.tb_lineno if getattr(e, "__traceback__", None) else -1
        except Exception:
            lineno = -1
        print(f"æ¨¡å¼1åˆ†å—é¢„æµ‹ä¸»å‡½æ•°å¤±è´¥: {str(e)} é”™è¯¯è¡Œ {lineno}")
        return ChunkedPredictionResponse(
            stock_code=request.stock_code,
            total_chunks=0,
            horizon_len=request.horizon_len,
            context_len=request.context_len,
            chunk_results=[],
            overall_metrics={'avg_mse': float('inf'), 'avg_mae': float('inf'), 'error': str(e)},
            processing_time=processing_time,
            validation_chunk_results=None,
        )

async def predict_validation_chunks_only(
        request: ChunkedPredictionRequest,
        tfm = None,
        timesfm_version: str = "2.0",
        fixed_best_prediction_item: Optional[str] = None,
        persist_best: bool = True,
        persist_val_chunks: bool = True,
    ) -> ChunkedPredictionResponse:
    """
    ä»…é¢„æµ‹éªŒè¯é›†åˆ†å—ï¼Œå¹¶ä½¿ç”¨å·²çŸ¥çš„æœ€ä½³åˆ†ä½æ•°ï¼ˆæ¥è‡ªJSONæˆ–ç¯å¢ƒå˜é‡ï¼‰ã€‚

    ç”¨é€”ï¼šå½“å·²å­˜åœ¨æœ€ä½³åˆ†ä½æ•°ï¼Œä½†æ²¡æœ‰ç¼“å­˜çš„åˆ†å—å“åº”æ—¶ï¼Œä»…é¢„æµ‹éªŒè¯é›†ä»¥è¿›è¡Œå›æµ‹ï¼Œæ— éœ€å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ã€‚

    Returns:
        ChunkedPredictionResponse: chunk_resultsä¸ºç©ºï¼›validation_chunk_resultsåŒ…å«éªŒè¯é›†åˆ†å—é¢„æµ‹ç»“æœï¼›
        overall_metricsä¸­åŒ…å«best_prediction_itemä¸éªŒè¯é›†æŒ‡æ ‡ã€‚
    """
    import time
    start_time = time.time()

    try:
        # æ•°æ®é¢„å¤„ç†
        df_original, df_train, df_test, df_val = await df_preprocess(
            request.stock_code,
            request.stock_type,
            request.start_date,
            request.end_date,
            request.time_step,
            years=request.years,
            horizon_len=request.horizon_len,
        )

        if df_original is None or df_train is None or df_test is None or df_val is None:
            print(f"âŒ è‚¡ç¥¨ {request.stock_code} æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼Œæ— æ³•è¿›è¡ŒéªŒè¯é›†é¢„æµ‹")
            return ChunkedPredictionResponse(
                stock_code=request.stock_code,
                total_chunks=0,
                horizon_len=request.horizon_len,
                chunk_results=[],
                overall_metrics={
                    'avg_mse': float('inf'),
                    'avg_mae': float('inf'),
                    'error': 'Data preprocessing failed'
                },
                processing_time=time.time() - start_time
            )

        print(f"âœ… è‚¡ç¥¨ {request.stock_code} æ•°æ®é¢„å¤„ç†æˆåŠŸï¼ˆéªŒè¯é›†ä¸“ç”¨æ¨¡å¼ï¼‰")
        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: è®­ç»ƒé›†={len(df_train)}, æµ‹è¯•é›†={len(df_test)}, éªŒè¯é›†={len(df_val)}")

        # æ·»åŠ å”¯ä¸€æ ‡è¯†ç¬¦
        df_train["unique_id"] = df_train["stock_code"].astype(str)
        df_test["unique_id"] = df_test["stock_code"].astype(str)
        df_val["unique_id"] = df_val["stock_code"].astype(str)

        # å¯¹éªŒè¯é›†è¿›è¡Œåˆ†å—
        val_chunks = create_chunks_from_test_data(df_val, request.horizon_len)
        val_results: List[ChunkPredictionResult] = []
        tqdm_bar = tqdm(total=len(val_chunks), desc="å¤„ç†éªŒè¯é›†åˆ†å—")
        for i, val_chunk in enumerate(val_chunks):
            tqdm_bar.update(1)
            tqdm_bar.set_description(f"å¤„ç†éªŒè¯é›†åˆ†å— {i+1}/{len(val_chunks)}")
            tqdm_bar.refresh()
            history_len = i * request.horizon_len
            if history_len > 0:
                cumulative_train_data = pd.concat([df_train, df_test, df_val.iloc[:history_len, :]], axis=0)
            else:
                cumulative_train_data = pd.concat([df_train, df_test], axis=0)

            val_result = predict_single_chunk_mode1(
                df_train=cumulative_train_data,
                df_test=val_chunk,
                tfm=tfm,
                chunk_index=i,
                request=request,
            )
            val_results.append(val_result)

        # è®¡ç®—éªŒè¯é›†æŒ‡æ ‡ï¼ˆä½¿ç”¨å›ºå®šæœ€ä½³åˆ†ä½æ•°ï¼‰
        validation_results = None
        if fixed_best_prediction_item:
            val_mse = []
            val_mae = []
            val_returns = []
            val_mle = []

            # ä½¿ç”¨è®­ç»ƒé›†æœ€åä¸€æ¡çš„æ”¶ç›˜ä»·ä½œä¸ºæ”¶ç›Šå¯¹æ¯”çš„åŸºå‡†ï¼Œä¸ä¸»æµç¨‹ä¸€è‡´
            try:
                df_train_last_one = df_train.iloc[-1, :]
            except Exception:
                df_train_last_one = None

            for result in val_results:
                if fixed_best_prediction_item in result.predictions:
                    pred_values = result.predictions[fixed_best_prediction_item]
                    actual_values = result.actual_values

                    mse = mean_squared_error(actual_values, pred_values)
                    mae = mean_absolute_error(actual_values, pred_values)
                    val_mse.append(mse)
                    val_mae.append(mae)

                    if len(pred_values) >= 1 and len(actual_values) >= 1:
                        try:
                            base_price = float(df_train_last_one['close']) if (df_train_last_one is not None and 'close' in df_train_last_one) else actual_values[0]
                        except Exception:
                            base_price = actual_values[0]
                        if not base_price or base_price == 0:
                            base_price = actual_values[0]
                        pred_return = (pred_values[-1] - base_price) / base_price * 100
                        actual_return = (actual_values[-1] - base_price) / base_price * 100
                        val_returns.append(abs(pred_return - actual_return))

                    min_len = min(len(pred_values), len(actual_values))
                    if min_len > 0:
                        residuals = np.array(actual_values[:min_len], dtype=float) - np.array(pred_values[:min_len], dtype=float)
                        mle_val = float(np.sqrt(np.mean(residuals ** 2)))
                        val_mle.append(mle_val)

            validation_results = {
                'best_prediction_item': fixed_best_prediction_item,
                'validation_mse': np.mean(val_mse) if val_mse else float('inf'),
                'validation_mae': np.mean(val_mae) if val_mae else float('inf'),
                'validation_return_diff': np.mean(val_returns) if val_returns else float('inf'),
                'validation_mle': np.mean(val_mle) if val_mle else float('inf'),
                'validation_chunks': len(val_results),
                'successful_validation_chunks': len(val_mse),
            }
            print(
                f"âœ… éªŒè¯ç»“æœ: MSE={validation_results['validation_mse']:.4f}, "
                f"MAE={validation_results['validation_mae']:.4f}, "
                f"MLE={validation_results['validation_mle']:.4f}, "
                f"æ¶¨è·Œå¹…å·®å¼‚={validation_results['validation_return_diff']:.2f}%"
            )

        # ä»…éªŒè¯æ¨¡å¼ä¸‹çš„æŒä¹…åŒ–ï¼šé¢„å…ˆå†™å…¥timesfm-bestï¼Œé¿å…åˆ†å—å¤–é”®å¤±è´¥
        pg = None
        saved_best_ok = False
        try:
            base_url = os.environ.get('POSTGRES_API', 'http://go-api.meetlife.com.cn:8000')
            pg = PostgresHandler(base_url=base_url, api_token="fintrack-dev-token")
            await pg.open()
            if fixed_best_prediction_item and persist_best:
                timesfm_version_str = timesfm_version
                def to_date_str(val):
                    try:
                        dt = pd.to_datetime(val, errors='coerce')
                        return dt.strftime('%Y-%m-%d') if not pd.isna(dt) else str(val)
                    except Exception:
                        return str(val)

                train_start_date = to_date_str(df_train['ds'].min())
                train_end_date = to_date_str(df_train['ds'].max())
                test_start_date = to_date_str(df_test['ds'].min())
                test_end_date = to_date_str(df_test['ds'].max())
                val_start_date = to_date_str(df_val['ds'].min())
                val_end_date = to_date_str(df_val['ds'].max())

                unique_key_best = f"{request.stock_code}_best_hlen_{request.horizon_len}_clen_{request.context_len}_v_{timesfm_version_str}"

                best_metrics_payload = validation_results if validation_results else {
                    'best_prediction_item': fixed_best_prediction_item
                }
                go_payload = {
                    "unique_key": unique_key_best,
                    "symbol": request.stock_code,
                    "timesfm_version": timesfm_version_str,
                    "best_prediction_item": fixed_best_prediction_item,
                    "best_metrics": _round_obj(best_metrics_payload),
                    "train_start_date": train_start_date,
                    "train_end_date": train_end_date,
                    "test_start_date": test_start_date,
                    "test_end_date": test_end_date,
                    "val_start_date": val_start_date,
                    "val_end_date": val_end_date,
                    "context_len": int(request.context_len),
                    "horizon_len": int(request.horizon_len),
                    "user_id": getattr(request, 'user_id', None),
                    "is_public": 1 if getattr(request, 'user_id', None) == 1 else 0,
                }

                status_code, data, body_text = await pg.save_best_prediction(go_payload)
                if status_code == 200:
                    print(f"âœ… å·²é€šè¿‡Goåç«¯ä¿å­˜timesfm-best(ä»…éªŒè¯æ¨¡å¼): unique_key={unique_key_best}")
                    saved_best_ok = True
                else:
                    print(f"âš ï¸ ä¿å­˜timesfm-bestå¤±è´¥(ä»…éªŒè¯æ¨¡å¼): status={status_code}, body={body_text}")
        except Exception as go_err:
            print(f"âš ï¸ ä»…éªŒè¯æ¨¡å¼è°ƒç”¨Goåç«¯ä¿å­˜bestå¤±è´¥: {go_err}")

        # å°†éªŒè¯é›†åˆ†å—é€å—å†™å…¥åç«¯ï¼ˆä»…éªŒè¯æ¨¡å¼ä¹ŸæŒä¹…åŒ–ï¼‰
        try:
            if val_results and persist_val_chunks and pg is not None:
                base_url = os.environ.get('POSTGRES_API', 'http://go-api.meetlife.com.cn:8000')
                timesfm_version_str = timesfm_version
                unique_key_val = f"{request.stock_code}_best_hlen_{request.horizon_len}_clen_{request.context_len}_v_{timesfm_version_str}"

                best_confirmed = saved_best_ok
                if not best_confirmed:
                    try:
                        status_code, data, body_text = await pg.get_best_by_unique(unique_key_val)
                        best_confirmed = (status_code == 200)
                    except Exception as chk_err:
                        best_confirmed = False
                if not best_confirmed:
                    try:
                        if fixed_best_prediction_item:
                            def to_date_str(val):
                                try:
                                    dt = pd.to_datetime(val, errors='coerce')
                                    return dt.strftime('%Y-%m-%d') if not pd.isna(dt) else str(val)
                                except Exception:
                                    return str(val)

                            train_start_date = to_date_str(df_train['ds'].min())
                            train_end_date = to_date_str(df_train['ds'].max())
                            test_start_date = to_date_str(df_test['ds'].min())
                            test_end_date = to_date_str(df_test['ds'].max())
                            val_start_date = to_date_str(df_val['ds'].min())
                            val_end_date = to_date_str(df_val['ds'].max())

                            best_metrics_payload = validation_results if validation_results else {
                                'best_prediction_item': fixed_best_prediction_item
                            }
                            go_payload = {
                                "unique_key": unique_key_val,
                                "symbol": request.stock_code,
                                "timesfm_version": timesfm_version_str,
                                "best_prediction_item": fixed_best_prediction_item,
                                "best_metrics": _round_obj(best_metrics_payload),
                                "train_start_date": train_start_date,
                                "train_end_date": train_end_date,
                                "test_start_date": test_start_date,
                                "test_end_date": test_end_date,
                                "val_start_date": val_start_date,
                                "val_end_date": val_end_date,
                                "context_len": int(request.context_len),
                                "horizon_len": int(request.horizon_len),
                                "user_id": getattr(request, 'user_id', None),
                                "is_public": 1 if getattr(request, 'user_id', None) == 1 else 0,
                            }
                            status_code, data, body_text = await pg.save_best_prediction(go_payload)
                            best_confirmed = (status_code == 200)
                            if best_confirmed:
                                print(f"âœ… å·²è¡¥å†™timesfm-best: unique_key={unique_key_val}")
                            else:
                                print(f"âš ï¸ è¡¥å†™timesfm-bestå¤±è´¥: status={status_code}, body={body_text}")
                    except Exception as add_err:
                        print(f"âš ï¸ å°è¯•è¡¥å†™timesfm-bestå¼‚å¸¸: {add_err}")
                if not best_confirmed:
                    print(f"âš ï¸ è·³è¿‡éªŒè¯åˆ†å—å†™å…¥ï¼šæœªæ‰¾åˆ°timesfm-best(unique_key={unique_key_val})ï¼Œé¿å…å¤–é”®å†²çª")
                    raise Exception("missing_best_record_for_val_chunks")

                for vcr in val_results:
                    try:
                        start_date = str(vcr.chunk_start_date)
                        end_date = str(vcr.chunk_end_date)
                        size = len(vcr.actual_values)
                        if size <= 0:
                            continue

                        chunk_dates = pd.date_range(
                            start=pd.to_datetime(start_date, errors='coerce'),
                            end=pd.to_datetime(end_date, errors='coerce'),
                            freq='D'
                        )[:size]
                        dates_str = [d.strftime('%Y-%m-%d') for d in chunk_dates]

                        def to_float4_list(arr):
                            out = []
                            for x in arr:
                                try:
                                    out.append(round(float(x), 4))
                                except Exception:
                                    out.append(None)
                            return out

                        predictions_clean = {}
                        preds_map = (vcr.predictions or {})
                        best_key = fixed_best_prediction_item
                        if best_key and best_key in preds_map:
                            predictions_clean[best_key] = to_float4_list(preds_map.get(best_key) or [])
                        else:
                            fallback_key = best_key or "mtf-0.5"
                            if fallback_key in preds_map:
                                predictions_clean[fallback_key] = to_float4_list(preds_map.get(fallback_key) or [])
                            else:
                                for k, arr in preds_map.items():
                                    predictions_clean[k] = to_float4_list(arr or [])
                                    break
                        actual_clean = to_float4_list(vcr.actual_values or [])

                        chunk_payload = {
                            "unique_key": unique_key_val,
                            "chunk_index": int(vcr.chunk_index),
                            "start_date": start_date,
                            "end_date": end_date,
                            "predictions": predictions_clean,
                            "actual_values": actual_clean,
                            "dates": dates_str,
                            "symbol": request.stock_code,
                            "is_public": 1 if getattr(request, 'user_id', None) == 1 else 0,
                            "user_id": getattr(request, 'user_id', None),
                        }

                        status_code, data, body_text = await pg.save_best_val_chunk(_round_obj(chunk_payload))
                        if status_code == 0:
                            print(f"âš ï¸ éªŒè¯åˆ†å—å†™å…¥å¤±è´¥(chunk={vcr.chunk_index})ï¼Œç½‘ç»œå¼‚å¸¸")
                            continue

                        if status_code == 200:
                            print(f"âœ… éªŒè¯åˆ†å—å·²ä¿å­˜: unique_key={unique_key_val}, chunk_index={vcr.chunk_index}")
                        else:
                            print(f"âš ï¸ éªŒè¯åˆ†å—ä¿å­˜å¤±è´¥: chunk={vcr.chunk_index}, status={status_code}, body={body_text}")
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†éªŒè¯åˆ†å—å†™å…¥å¼‚å¸¸(chunk={getattr(vcr,'chunk_index', '?')}): {e}")
        except Exception as e:
            print(f"âš ï¸ éªŒè¯åˆ†å—å†™å…¥åç«¯è¿‡ç¨‹å¼‚å¸¸: {e}")
        finally:
            try:
                if pg is not None:
                    await pg.close()
            except Exception:
                pass
        if not fixed_best_prediction_item:
            print("âš ï¸ æœªæä¾›å›ºå®šæœ€ä½³åˆ†ä½æ•°ï¼ŒéªŒè¯é›†æŒ‡æ ‡æ— æ³•è®¡ç®—ï¼Œoverall_metricsä»…åŒ…å«éªŒè¯åˆ†å—æ•°é‡")

        overall_metrics = {
            'best_prediction_item': fixed_best_prediction_item,
            'validation_results': validation_results,
            'total_chunks': len(val_chunks),
            'successful_chunks': len(val_results),
        }

        processing_time = time.time() - start_time

        resp = ChunkedPredictionResponse(
            stock_code=request.stock_code,
            total_chunks=len(val_chunks),
            horizon_len=request.horizon_len,
            context_len=request.context_len,
            chunk_results=[],
            overall_metrics=overall_metrics,
            processing_time=processing_time,
            concatenated_predictions=None,
            concatenated_actual=None,
            concatenated_dates=None,
            validation_chunk_results=val_results if val_results else None,
        )

        # ä¿å­˜å“åº”åˆ°JSONï¼Œä¾¿äºå›æµ‹ç›´æ¥åŠ è½½
        try:
            out_dir = os.path.join(finance_dir, "forecast-results")
            os.makedirs(out_dir, exist_ok=True)
            out_path = os.path.join(out_dir, f"{request.stock_code}_chunked_response.json")

            def _cr_to_dict(cr: ChunkPredictionResult):
                return {
                    "chunk_index": cr.chunk_index,
                    "chunk_start_date": cr.chunk_start_date,
                    "chunk_end_date": cr.chunk_end_date,
                    "predictions": cr.predictions,
                    "actual_values": cr.actual_values,
                    "metrics": cr.metrics,
                }

            payload = {
                "stock_code": resp.stock_code,
                "total_chunks": resp.total_chunks,
                "horizon_len": resp.horizon_len,
                "chunk_results": [],
                "overall_metrics": resp.overall_metrics,
                "processing_time": resp.processing_time,
                "concatenated_predictions": resp.concatenated_predictions,
                "concatenated_actual": resp.concatenated_actual,
                "concatenated_dates": resp.concatenated_dates,
                "validation_chunk_results": [ _cr_to_dict(vcr) for vcr in (resp.validation_chunk_results or []) ] if resp.validation_chunk_results else None,
                "is_public": 1,
                "user_id": 1,
            }
            with open(out_path, "w", encoding="utf-8") as f:
                json.dump(_round_obj(payload), f, ensure_ascii=False, indent=2)
            print(f"âœ… éªŒè¯é›†åˆ†å—å“åº”å·²ä¿å­˜: {out_path}")
        except Exception as save_err:
            print(f"âš ï¸ ä¿å­˜éªŒè¯é›†åˆ†å—å“åº” JSON å¤±è´¥: {save_err}")

        return resp
    except Exception as e:
        processing_time = time.time() - start_time
        print(f"éªŒè¯é›†åˆ†å—é¢„æµ‹å¤±è´¥: {str(e)} é”™è¯¯è¡Œ {e.__traceback__.tb_lineno}")
        return ChunkedPredictionResponse(
            stock_code=request.stock_code,
            total_chunks=0,
            horizon_len=request.horizon_len,
            context_len=request.context_len,
            chunk_results=[],
            overall_metrics={'avg_mse': float('inf'), 'avg_mae': float('inf'), 'error': str(e)},
            processing_time=processing_time,
            validation_chunk_results=None,
        )
        
    except Exception as e:
        processing_time = time.time() - start_time
        print(f"åˆ†å—é¢„æµ‹å¤±è´¥: {str(e)} é”™è¯¯è¡Œ {e.__traceback__.tb_lineno}")
        
        return ChunkedPredictionResponse(
            stock_code=request.stock_code,
            total_chunks=0,
            horizon_len=request.horizon_len,
            context_len=request.context_len,
            chunk_results=[],
            overall_metrics={'avg_mse': float('inf'), 'avg_mae': float('inf'), 'error': str(e)},
            processing_time=processing_time
        )

def main():
    import asyncio
    # from timesfm_init import init_timesfm
    test_request = ChunkedPredictionRequest(
        stock_code="sz000001",
        years=15,
        horizon_len=3,
        start_date="",
        end_date="20251201",
        context_len=2048,
        time_step=0,
        stock_type=2,
        timesfm_version="2.5",
        user_id=1
    )
    if test_request.timesfm_version == "2.0":
        # tfm = init_timesfm(horizon_len=test_request.horizon_len, context_len=test_request.context_len)
        response = asyncio.run(predict_chunked_mode_for_best(test_request))
    else:
        response = asyncio.run(predict_chunked_mode_for_best(test_request))
    # print(response)
    # è¾“å‡ºç»“æœ
    print(f"\n=== åˆ†å—é¢„æµ‹ç»“æœ ===")
    print(f"è‚¡ç¥¨ä»£ç : {response.stock_code}")
    print(f"æ€»åˆ†å—æ•°: {response.total_chunks}")
    print(f"é¢„æµ‹é•¿åº¦: {response.horizon_len}")
    print(f"ä¸Šä¸‹æ–‡é•¿åº¦: {response.context_len}")
    print(f"å¤„ç†æ—¶é—´: {response.processing_time:.2f} ç§’")
    print(f"å¤„ç†ç»“æœ: {response.overall_metrics}")
    # ç”Ÿæˆç»˜å›¾
    from plot_functions import plot_chunked_prediction_results
    plot_save_path = os.path.join(finance_dir, f"forecast-results/{test_request.stock_code}_prediction_plot.png")
    try:
        plot_path = plot_chunked_prediction_results(response, plot_save_path)
    except Exception as plot_error:
        print(f"âš ï¸ ç»˜å›¾å¤±è´¥: {str(plot_error)}")
    
def test_next_chunked_prediction():
    import asyncio
    res = asyncio.run(predict_next_chunk_by_unique_key(
        unique_key="sz000001_best_hlen_3_clen_2048_v_2.5",
        user_id=1,
        persist=True,
    ))
    print(res)

if __name__ == "__main__":
    # main()
    test_next_chunked_prediction()

    
